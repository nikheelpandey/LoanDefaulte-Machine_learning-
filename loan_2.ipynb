{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use('ggplot')\n",
    "train_df = pd.read_csv('train_indessa.csv')\n",
    "test_df = pd.read_csv('test_indessa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_df = train_df.drop(['batch_enrolled','verification_status','application_type','mths_since_last_record','verification_status_joint','mths_since_last_major_derog','desc','pymnt_plan','mths_since_last_delinq'], axis=1)\n",
    "test_df = test_df.drop(['batch_enrolled','verification_status','application_type','mths_since_last_record','verification_status_joint','mths_since_last_major_derog','desc','pymnt_plan','mths_since_last_delinq'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "member_id                         0\n",
       "loan_amnt                         0\n",
       "funded_amnt                       0\n",
       "funded_amnt_inv                   0\n",
       "term                              0\n",
       "int_rate                          0\n",
       "grade                             0\n",
       "sub_grade                         0\n",
       "emp_title                     30830\n",
       "emp_length                        0\n",
       "home_ownership                    0\n",
       "annual_inc                        3\n",
       "purpose                           0\n",
       "title                            90\n",
       "zip_code                          0\n",
       "addr_state                        0\n",
       "dti                               0\n",
       "delinq_2yrs                      16\n",
       "inq_last_6mths                   16\n",
       "open_acc                         16\n",
       "pub_rec                          16\n",
       "revol_bal                         0\n",
       "revol_util                      287\n",
       "total_acc                        16\n",
       "initial_list_status               0\n",
       "total_rec_int                     0\n",
       "total_rec_late_fee                0\n",
       "recoveries                        0\n",
       "collection_recovery_fee           0\n",
       "collections_12_mths_ex_med       95\n",
       "last_week_pay                     0\n",
       "acc_now_delinq                   16\n",
       "tot_coll_amt                  42004\n",
       "tot_cur_bal                   42004\n",
       "total_rev_hi_lim              42004\n",
       "loan_status                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = 0\n",
    "train_df['zip_code'] = (train_df.zip_code.str.extract('(\\d+)', expand=True))\n",
    "test_df['zip_code'] = (test_df.zip_code.str.extract('(\\d+)', expand=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "th = 0\n",
    "train_df['last_week_pay'] = (train_df.zip_code.str.extract('(\\d+)', expand=True))\n",
    "test_df['last_week_pay'] = (test_df.zip_code.str.extract('(\\d+)', expand=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.fillna(0)\n",
    "test_df = test_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#pd.crosstab(train_df[('last_week_pay')],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "title_mapping = {'1 year':1,\n",
    "'10+ years':10,\n",
    "'2 years':2,\n",
    "'3 years':3,\n",
    "'4 years':4,\n",
    "'5 years':5,\n",
    "'6 years':6,\n",
    "'7 years':7,\n",
    "'8 years':8,\n",
    "'9 years':9,\n",
    "'< 1 year':0}\n",
    "\n",
    "train_df['emp_length']= train_df['emp_length'].map(title_mapping)\n",
    "test_df['emp_length']= test_df['emp_length'].map(title_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "title_mapping = {\"AK\":0, \n",
    "\"AL\":1, \n",
    "\"AR\" :2,\n",
    "\"AZ\" :3,\n",
    "\"CA \":4,\n",
    "\"CO\" :5,\n",
    "\"CT\" :6,\n",
    "\"DC\":7,\n",
    "\"DE\" :8,\n",
    "\"FL\":9,\n",
    "\"GA\" :10,\n",
    "\"HI\" :11,\n",
    "\"IA\":12,\n",
    "\"ID\" :13,\n",
    "\"IL\":14,\n",
    "\"IN\" :15,\n",
    "\"KS\":16,\n",
    "\"KY\" :17,\n",
    "\"LA\":18,\n",
    "\"MA\":19,\n",
    "\"MD\":20,\n",
    "\"ME\" :21,\n",
    "\"MI\":22,\n",
    "\"MN\":23,\n",
    "\"MO\" :24,\n",
    "\"MS\" :25,\n",
    "\"MT\" :26,\n",
    "\"NC\":27,\n",
    "\"ND\" :28,\n",
    "\"NE\":29,\n",
    "\"NH\" :30,\n",
    "\"NJ\":31,\n",
    "\"NM\":32,\n",
    "\"NV\":33,\n",
    "\"NY\":34,\n",
    "\"OH\":35,\n",
    "\"OK\":36,\n",
    "\"OR\":37,\n",
    "\"PA\" :38,\n",
    "\"RI\" :39,\n",
    "\"SC\" :40,\n",
    "\"SD\" :41,\n",
    "\"TN\" :42,\n",
    "\"TX\" :43,\n",
    "\"UT\" :44,\n",
    "\"VA\" :45,\n",
    "\"VT\" :46,\n",
    "\"WA\" :47,\n",
    "\"WI\" :48,\n",
    "\"WV\" :49,\n",
    "\"WY\":50}\n",
    "\n",
    "train_df['addr_state']= train_df['addr_state'].map(title_mapping)\n",
    "test_df['addr_state']= test_df['addr_state'].map(title_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "title_mapping = {\"36 months\":0, \"60 months\":1}\n",
    "\n",
    "train_df['term']= train_df['term'].map(title_mapping)\n",
    "test_df['term']= test_df['term'].map(title_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    532428.000000\n",
       "mean         18.138767\n",
       "std           8.369074\n",
       "min           0.000000\n",
       "25%          11.930000\n",
       "50%          17.650000\n",
       "75%          23.950000\n",
       "max         672.520000\n",
       "Name: dti, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_mapping = {\"A\":0, \"B\":1, \"C\":2,\"D\":3,\"E\":4,\"F\":5,\"G\":6}\n",
    "\n",
    "train_df['grade']= train_df['grade'].map(title_mapping)\n",
    "test_df['grade']= test_df['grade'].map(title_mapping)\n",
    " \n",
    "train_df.dti.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "title_mapping = {\"ANY\":0,\"MORTGAGE\":1,\"NONE\":2,\"OTHER\":3,\"OWN\":4,\"RENT\":5}\n",
    "train_df['home_ownership']= train_df['home_ownership'].map(title_mapping)\n",
    "test_df['home_ownership']= test_df['home_ownership'].map(title_mapping)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "title_mapping = {\"car\":0, \"credit_card\":1,\"debt_consolidation\":2,\"educational\":3,\"home_improvement\":4,\"major_purchase\":5,\"medical\":6,\"moving\":7,\"other\":8,\"renewable_energy\":9,\"small_business\":10,\"vacation\":11,\"wedding\":12,\"house\":13}\n",
    "train_df['purpose']= train_df['purpose'].map(title_mapping)\n",
    "test_df['purpose']= test_df['purpose'].map(title_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "title_mapping = {\"f\":0, \"w\":1}\n",
    "train_df['initial_list_status']= train_df['initial_list_status'].map(title_mapping)\n",
    "test_df['initial_list_status']= test_df['initial_list_status'].map(title_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "combine=[ train_df,test_df]\n",
    "for dataset in combine:\n",
    "    dataset['delinq_2yrs'] = dataset['delinq_2yrs'].fillna(dataset['delinq_2yrs'].median())\n",
    "    dataset['annual_inc'] = dataset['annual_inc'].fillna(dataset['annual_inc'].median())\n",
    "    dataset['inq_last_6mths'] = dataset['inq_last_6mths'].fillna(dataset['inq_last_6mths'].median())\n",
    "    #dataset['mths_since_last_delinq'] = dataset['mths_since_last_delinq'].fillna(dataset['mths_since_last_delinq'].median())\n",
    "    dataset['dti'] = dataset['dti'].fillna(dataset['dti'].median())\n",
    "    dataset['open_acc'] = dataset['open_acc'].fillna(dataset['open_acc'].median())\n",
    "    dataset['pub_rec'] = dataset['pub_rec'].fillna(dataset['pub_rec'].median())\n",
    "    dataset['revol_util'] = dataset['revol_util'].fillna(dataset['revol_util'].median())\n",
    "    dataset['total_acc'] = dataset['total_acc'].fillna(dataset['total_acc'].median())\n",
    "    dataset['emp_length'] = dataset['emp_length'].fillna(dataset['emp_length'].median())\n",
    "    dataset['tot_cur_bal'] = dataset['tot_cur_bal'].fillna(dataset['tot_cur_bal'].median())\n",
    "    dataset['total_rev_hi_lim'] = dataset['total_rev_hi_lim'].fillna(dataset['total_rev_hi_lim'].median())\n",
    "    #dataset['pymnt_plan'] = dataset['pymnt_plan'].fillna(dataset['pymnt_plan'].median())\n",
    "    dataset['addr_state'] = dataset['addr_state'].fillna(dataset['addr_state'].median())\n",
    "    #dataset['mths_since_last_major_derog'] = dataset['mths_since_last_major_derog'].fillna(dataset['mths_since_last_major_derog'].median())\n",
    "    #dataset['pymnt_plan'] = dataset['pymnt_plan'].fillna(3)\n",
    "    dataset['collections_12_mths_ex_med'] = dataset['collections_12_mths_ex_med'].fillna(dataset['collections_12_mths_ex_med'].median())\n",
    "    dataset['acc_now_delinq'] = dataset['acc_now_delinq'].fillna(dataset['acc_now_delinq'].median())\n",
    "    dataset['tot_coll_amt'] = dataset['tot_coll_amt'].fillna(dataset['tot_coll_amt'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_df[\"annual_inc\"] = np.log(train_df['annual_inc'])\n",
    "test_df[\"annual_inc\" ]=  np.log(train_df['annual_inc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ax1=plt.subplot(1,1,1)\n",
    "\n",
    "# train_df['annual_inc'].plot()\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "obj = int(train_df['annual_inc'].median())\n",
    "train_df=train_df[train_df['annual_inc']< 15]\n",
    "train_df=train_df[train_df['annual_inc'] > 8]\n",
    "#train_df=train_df[train_df['tot_cur_bal']< 2500000]\n",
    "# ax1=plt.subplot(1,1,1)\n",
    "\n",
    "# train_df['annual_inc'].plot()\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Encoded: ', 'addr_state')\n",
      "('Encoded: ', 'zip_code')\n",
      "('Encoded: ', 'last_week_pay')\n",
      "('Encoded: ', 'initial_list_status')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_cols = ['addr_state',\"emp_title\",\"title\",'zip_code','last_week_pay','initial_list_status']\n",
    "le = {}\n",
    "\n",
    "for col in cat_cols:\n",
    "    le[col] = LabelEncoder()\n",
    "    train_df[col] = le[col].fit_transform(train_df[col])\n",
    "    test_df[col] = le[col].fit_transform(test_df[col])\n",
    "    le[col].classes_ = np.append(le[col].classes_, 'other')\n",
    "    \n",
    "    print('Encoded: ', col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "member_id                     0\n",
       "loan_amnt                     0\n",
       "funded_amnt                   0\n",
       "funded_amnt_inv               0\n",
       "term                          0\n",
       "int_rate                      0\n",
       "grade                         0\n",
       "sub_grade                     0\n",
       "emp_length                    0\n",
       "home_ownership                0\n",
       "annual_inc                    0\n",
       "purpose                       0\n",
       "zip_code                      0\n",
       "addr_state                    0\n",
       "dti                           0\n",
       "delinq_2yrs                   0\n",
       "inq_last_6mths                0\n",
       "open_acc                      0\n",
       "pub_rec                       0\n",
       "revol_bal                     0\n",
       "revol_util                    0\n",
       "total_acc                     0\n",
       "initial_list_status           0\n",
       "total_rec_int                 0\n",
       "total_rec_late_fee            0\n",
       "recoveries                    0\n",
       "collection_recovery_fee       0\n",
       "collections_12_mths_ex_med    0\n",
       "last_week_pay                 0\n",
       "acc_now_delinq                0\n",
       "tot_coll_amt                  0\n",
       "tot_cur_bal                   0\n",
       "total_rev_hi_lim              0\n",
       "loan_status                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##droping a'verification_status',ll those attributes which do have great corelation with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#pd.crosstab(test_df['annual_inc'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "test_df = test_df.drop([\"funded_amnt\",\"funded_amnt_inv\",\"collection_recovery_fee\"], axis=1)\n",
    "train_df = train_df.drop([\"funded_amnt\",\"funded_amnt_inv\",\"collection_recovery_fee\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#train_df[\"dti\"] =  (train_df['dti']/train_df['dti'].mean())\n",
    "#test_df[\"dti\" ]=  (test_df['dti']/test_df['dti'].mean())\n",
    "#train_df[\"total_rev_hi_lim\"] = (train_df['total_rev_hi_lim']/(train_df['total_rev_hi_lim'].median()))\n",
    "#test_df[\"total_rev_hi_lim\" ]= (test_df['total_rev_hi_lim']/(train_df['total_rev_hi_lim'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#addition of new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#train_df[\"new_var_2\"] = np.log(train_df['annual_inc']/(train_df['loan_amnt']))\n",
    "#test_df[\"new_var_2\" ]= np.log(test_df['annual_inc']/(train_df['annual_inc']))\n",
    "#train_df[\"new_var_3\"] = (train_df['total_rec_int']+(train_df['total_rec_late_fee']))\n",
    "#test_df[\"new_var_3\" ]= (test_df['total_rec_int']+(train_df['total_rec_late_fee']))\n",
    "#train_df[\"new_var_3\"] = (train_df['loan_amnt']*(train_df['int_rate']))\n",
    "#test_df[\"new_var_3\" ]= (test_df['loan_amnt']*(train_df['int_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#train_df = train_df.drop(['annual_inc','loan_amnt','total_rec_int','total_rec_late_fee','loan_amnt','int_rate'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#test_df = test_df.drop(['annual_inc','loan_amnt','total_rec_int','total_rec_late_fee','loan_amnt','int_rate'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_dfx = train_df[50000:]\n",
    "test_dfx = train_df[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((482404, 29), (482404,), (50000, 29))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_dfx.drop([\"loan_status\", 'member_id'], axis=1)\n",
    "Y_train = train_dfx[\"loan_status\"]\n",
    "X_test  = test_dfx.drop([\"member_id\",\"loan_status\"], axis=1).copy()\n",
    "Y_test = test_dfx[\"loan_status\"]\n",
    "X_train.shape, Y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 25building tree 1 of 25\n",
      "\n",
      "building tree 4 of 25building tree 3 of 25\n",
      "\n",
      "building tree 5 of 25\n",
      "building tree 6 of 25\n",
      "building tree 7 of 25\n",
      "building tree 8 of 25\n",
      "building tree 9 of 25\n",
      "building tree 10 of 25\n",
      "building tree 11 of 25\n",
      "building tree 12 of 25\n",
      "building tree 13 of 25\n",
      "building tree 14 of 25\n",
      "building tree 15 of 25\n",
      "building tree 16 of 25\n",
      "building tree 17 of 25\n",
      "building tree 18 of 25\n",
      "building tree 19 of 25\n",
      "building tree 20 of 25\n",
      "building tree 21 of 25\n",
      "building tree 22 of 25\n",
      "building tree 23 of 25\n",
      "building tree 24 of 25\n",
      "building tree 25 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "86.29"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfETC = ExtraTreesClassifier(n_jobs = -1, n_estimators = 25, max_features = 28, verbose=2, min_samples_split = 25 ,warm_start = True)\n",
    "clfETC.fit(X_train, Y_train)\n",
    "acc_log = round(clfETC.score(X_test,Y_test) * 100,3)\n",
    "acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:    2.1s finished\n"
     ]
    }
   ],
   "source": [
    "X_test_t=test_df.drop([\"member_id\"], axis=1).copy()\n",
    "Y_pred = (clfETC.predict_proba(X_test_t))\n",
    "df = []\n",
    "for i in range(len(X_test_t)):\n",
    "    if ((Y_pred[i,1]) < (0.0001)): df.append(0.1)\n",
    "    elif ((Y_pred[i,1]) > (0.99)):df.append(0.99)\n",
    "    else: df.append((Y_pred[i,1]))\n",
    "member_id = test_df['member_id']     \n",
    "dfETC = pd.DataFrame( { 'member_id': member_id , 'loan_status': df } )\n",
    "dfETC = dfETC [['member_id', 'loan_status']]\n",
    "#df2.to_csv( 'loan_2.xb.csv', index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.322"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfxbg = XGBClassifier(max_depth=3, n_estimators=1000,silent=True)\n",
    "clfxbg.fit(X_train, Y_train)\n",
    "acc_log = round(clfxbg.score(X_test,Y_test) * 100,3)\n",
    "acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test_t=test_df.drop([\"member_id\"], axis=1).copy()\n",
    "Y_pred = (clfxbg.predict_proba(X_test_t))\n",
    "df = []\n",
    "for i in range(len(X_test_t)):\n",
    "    if ((Y_pred[i,1]) < (0.0001)): df.append(0.1)\n",
    "    elif ((Y_pred[i,1]) > (0.99)):df.append(0.99)\n",
    "    else: df.append((Y_pred[i,1]))\n",
    "member_id = test_df['member_id']     \n",
    "dfxbg = pd.DataFrame( { 'member_id': member_id , 'loan_status': df } )\n",
    "dfxbg = dfxbg [['member_id', 'loan_status']]\n",
    "#df2.to_csv( 'loan_2.xb.csv', index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 25building tree 2 of 25\n",
      "\n",
      "building tree 3 of 25\n",
      "building tree 4 of 25\n",
      "building tree 5 of 25\n",
      "building tree 6 of 25\n",
      "building tree 7 of 25\n",
      "building tree 8 of 25\n",
      "building tree 9 of 25\n",
      "building tree 10 of 25\n",
      "building tree 11 of 25\n",
      "building tree 12 of 25\n",
      "building tree 13 of 25\n",
      "building tree 14 of 25\n",
      "building tree 15 of 25\n",
      "building tree 16 of 25\n",
      "building tree 17 of 25\n",
      "building tree 18 of 25\n",
      "building tree 19 of 25\n",
      "building tree 20 of 25\n",
      "building tree 21 of 25\n",
      "building tree 22 of 25\n",
      "building tree 23 of 25\n",
      "building tree 24 of 25\n",
      "building tree 25 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87.266"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_jobs = -1, n_estimators = 25, max_features = 28, verbose=2, min_samples_split = 25 ,warm_start = True)\n",
    "clf.fit(X_train, Y_train)\n",
    "acc_log = round(clf.score(X_test,Y_test) * 100,3)\n",
    "Y_pred=(clf.predict_proba(X_test))\n",
    "acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:    1.9s finished\n"
     ]
    }
   ],
   "source": [
    "X_test_t=test_df.drop([\"member_id\"], axis=1).copy()\n",
    "Y_pred = (clf.predict_proba(X_test_t))\n",
    "df = []\n",
    "for i in range(len(X_test_t)):\n",
    "    if ((Y_pred[i,1]) < (0.0001)): df.append(0.1)\n",
    "    elif ((Y_pred[i,1]) > (0.99)):df.append(0.99)\n",
    "    else: df.append((Y_pred[i,1]))\n",
    "member_id = test_df['member_id']     \n",
    "dfrf = pd.DataFrame( { 'member_id': member_id , 'loan_status': df } )\n",
    "dfrf = dfrf[['member_id', 'loan_status']]\n",
    "#df2.to_csv( 'loan_2.xb.csv', index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.52399610\n",
      "Iteration 2, loss = 0.47951728\n",
      "Iteration 3, loss = 0.46580398\n",
      "Iteration 4, loss = 0.45627339\n",
      "Iteration 5, loss = 0.44868621\n",
      "Iteration 6, loss = 0.44214109\n",
      "Iteration 7, loss = 0.43585808\n",
      "Iteration 8, loss = 0.42944516\n",
      "Iteration 9, loss = 0.42273950\n",
      "Iteration 10, loss = 0.41600027\n",
      "Iteration 11, loss = 0.40981191\n",
      "Iteration 12, loss = 0.40396438\n",
      "Iteration 13, loss = 0.39839479\n",
      "Iteration 14, loss = 0.39409295\n",
      "Iteration 15, loss = 0.39038543\n",
      "Iteration 16, loss = 0.38729476\n",
      "Iteration 17, loss = 0.38436228\n",
      "Iteration 18, loss = 0.38173427\n",
      "Iteration 19, loss = 0.37949325\n",
      "Iteration 20, loss = 0.37734695\n",
      "Iteration 21, loss = 0.37543421\n",
      "Iteration 22, loss = 0.37368902\n",
      "Iteration 23, loss = 0.37213428\n",
      "Iteration 24, loss = 0.37069756\n",
      "Iteration 25, loss = 0.36938920\n",
      "Iteration 26, loss = 0.36809569\n",
      "Iteration 27, loss = 0.36686151\n",
      "Iteration 28, loss = 0.36569653\n",
      "Iteration 29, loss = 0.36486567\n",
      "Iteration 30, loss = 0.36389245\n",
      "Iteration 31, loss = 0.36303747\n",
      "Iteration 32, loss = 0.36214981\n",
      "Iteration 33, loss = 0.36115798\n",
      "Iteration 34, loss = 0.36033262\n",
      "Iteration 35, loss = 0.35950735\n",
      "Iteration 36, loss = 0.35863552\n",
      "Iteration 37, loss = 0.35783086\n",
      "Iteration 38, loss = 0.35692634\n",
      "Iteration 39, loss = 0.35622591\n",
      "Iteration 40, loss = 0.35538406\n",
      "Iteration 41, loss = 0.35467634\n",
      "Iteration 42, loss = 0.35395645\n",
      "Iteration 43, loss = 0.35325606\n",
      "Iteration 44, loss = 0.35258138\n",
      "Iteration 45, loss = 0.35195206\n",
      "Iteration 46, loss = 0.35130219\n",
      "Iteration 47, loss = 0.35046237\n",
      "Iteration 48, loss = 0.34971896\n",
      "Iteration 49, loss = 0.34910511\n",
      "Iteration 50, loss = 0.34823746\n",
      "Iteration 51, loss = 0.34752960\n",
      "Iteration 52, loss = 0.34690114\n",
      "Iteration 53, loss = 0.34599859\n",
      "Iteration 54, loss = 0.34537691\n",
      "Iteration 55, loss = 0.34458295\n",
      "Iteration 56, loss = 0.34381275\n",
      "Iteration 57, loss = 0.34320737\n",
      "Iteration 58, loss = 0.34242409\n",
      "Iteration 59, loss = 0.34157618\n",
      "Iteration 60, loss = 0.34100735\n",
      "Iteration 61, loss = 0.34002687\n",
      "Iteration 62, loss = 0.33932052\n",
      "Iteration 63, loss = 0.33866172\n",
      "Iteration 64, loss = 0.33780213\n",
      "Iteration 65, loss = 0.33700909\n",
      "Iteration 66, loss = 0.33614032\n",
      "Iteration 67, loss = 0.33544938\n",
      "Iteration 68, loss = 0.33463120\n",
      "Iteration 69, loss = 0.33399694\n",
      "Iteration 70, loss = 0.33328484\n",
      "Iteration 71, loss = 0.33241684\n",
      "Iteration 72, loss = 0.33170425\n",
      "Iteration 73, loss = 0.33082973\n",
      "Iteration 74, loss = 0.33019510\n",
      "Iteration 75, loss = 0.32937636\n",
      "Iteration 76, loss = 0.32893294\n",
      "Iteration 77, loss = 0.32801886\n",
      "Iteration 78, loss = 0.32731932\n",
      "Iteration 79, loss = 0.32666455\n",
      "Iteration 80, loss = 0.32600956\n",
      "Iteration 81, loss = 0.32536548\n",
      "Iteration 82, loss = 0.32458464\n",
      "Iteration 83, loss = 0.32393987\n",
      "Iteration 84, loss = 0.32335077\n",
      "Iteration 85, loss = 0.32267962\n",
      "Iteration 86, loss = 0.32214838\n",
      "Iteration 87, loss = 0.32156204\n",
      "Iteration 88, loss = 0.32121879\n",
      "Iteration 89, loss = 0.32080624\n",
      "Iteration 90, loss = 0.32004612\n",
      "Iteration 91, loss = 0.31957032\n",
      "Iteration 92, loss = 0.31919508\n",
      "Iteration 93, loss = 0.31874672\n",
      "Iteration 94, loss = 0.31840329\n",
      "Iteration 95, loss = 0.31790392\n",
      "Iteration 96, loss = 0.31757457\n",
      "Iteration 97, loss = 0.31705696\n",
      "Iteration 98, loss = 0.31680388\n",
      "Iteration 99, loss = 0.31651378\n",
      "Iteration 100, loss = 0.31624242\n",
      "Iteration 101, loss = 0.31566191\n",
      "Iteration 102, loss = 0.31538178\n",
      "Iteration 103, loss = 0.31519681\n",
      "Iteration 104, loss = 0.31501273\n",
      "Iteration 105, loss = 0.31466575\n",
      "Iteration 106, loss = 0.31449613\n",
      "Iteration 107, loss = 0.31402742\n",
      "Iteration 108, loss = 0.31375355\n",
      "Iteration 109, loss = 0.31352160\n",
      "Iteration 110, loss = 0.31315761\n",
      "Iteration 111, loss = 0.31309627\n",
      "Iteration 112, loss = 0.31267687\n",
      "Iteration 113, loss = 0.31427033\n",
      "Iteration 114, loss = 0.31264397\n",
      "Iteration 115, loss = 0.31214114\n",
      "Iteration 116, loss = 0.31206293\n",
      "Iteration 117, loss = 0.31176908\n",
      "Iteration 118, loss = 0.31157762\n",
      "Iteration 119, loss = 0.31145363\n",
      "Iteration 120, loss = 0.31124102\n",
      "Iteration 121, loss = 0.31082646\n",
      "Iteration 122, loss = 0.31058088\n",
      "Iteration 123, loss = 0.31084812\n",
      "Iteration 124, loss = 0.31041125\n",
      "Iteration 125, loss = 0.31015090\n",
      "Iteration 126, loss = 0.30986916\n",
      "Iteration 127, loss = 0.30987357\n",
      "Iteration 128, loss = 0.30956708\n",
      "Iteration 129, loss = 0.30949191\n",
      "Iteration 130, loss = 0.30915767\n",
      "Iteration 131, loss = 0.30904266\n",
      "Iteration 132, loss = 0.30884159\n",
      "Iteration 133, loss = 0.30877040\n",
      "Iteration 134, loss = 0.30857986\n",
      "Iteration 135, loss = 0.30853092\n",
      "Iteration 136, loss = 0.30889817\n",
      "Iteration 137, loss = 0.30805870\n",
      "Iteration 138, loss = 0.30806904\n",
      "Iteration 139, loss = 0.30809732\n",
      "Iteration 140, loss = 0.30811419\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 141, loss = 0.30502587\n",
      "Iteration 142, loss = 0.30428381\n",
      "Iteration 143, loss = 0.30419927\n",
      "Iteration 144, loss = 0.30407648\n",
      "Iteration 145, loss = 0.30418008\n",
      "Iteration 146, loss = 0.30411496\n",
      "Iteration 147, loss = 0.30395485\n",
      "Iteration 148, loss = 0.30393114\n",
      "Iteration 149, loss = 0.30408922\n",
      "Iteration 150, loss = 0.30391208\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 151, loss = 0.30328530\n",
      "Iteration 152, loss = 0.30312445\n",
      "Iteration 153, loss = 0.30309883\n",
      "Iteration 154, loss = 0.30309990\n",
      "Iteration 155, loss = 0.30308171\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 156, loss = 0.30290490\n",
      "Iteration 157, loss = 0.30289371\n",
      "Iteration 158, loss = 0.30288333\n",
      "Iteration 159, loss = 0.30288036\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 160, loss = 0.30284219\n",
      "Iteration 161, loss = 0.30283802\n",
      "Iteration 162, loss = 0.30283645\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 163, loss = 0.30282576\n",
      "Iteration 164, loss = 0.30282545\n",
      "Iteration 165, loss = 0.30282528\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(40,20,10),warm_start=2, verbose= 2,solver = 'sgd',learning_rate ='adaptive', )\n",
    "mlp.fit(X_train,Y_train)\n",
    "acc_log = round(mlp.score(X_test,Y_test) * 100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.22"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test_t  = test_df.drop([\"member_id\"], axis=1).copy()\n",
    "Y_pred = (mlp.predict_proba(X_test_t))\n",
    "df = []\n",
    "for i in range(len(X_test_t)):\n",
    "    if ((Y_pred[i,1]) < (0.0001)): df.append(0.1)\n",
    "    elif ((Y_pred[i,1]) > (0.99)):df.append(0.99)\n",
    "    else: df.append((Y_pred[i,1]))\n",
    "member_id = test_df['member_id']     \n",
    "dfmlp = pd.DataFrame( { 'member_id': member_id , 'loan_status': df } )\n",
    "dfmlp = dfmlp [['member_id', 'loan_status']]\n",
    "#df2.to_csv( 'loan_2.xb.csv', index = False )\n",
    "#train_df.to_csv( 'train_df.csv', index = False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.214"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clflr = LogisticRegression(n_jobs=-1,verbose=2)\n",
    "clflr.fit(X_train, Y_train)\n",
    "acc_log = round(clflr.score(X_test,Y_test) * 100,3)\n",
    "acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test_t=test_df.drop([\"member_id\"], axis=1).copy()\n",
    "Y_pred = (clflr.predict_proba(X_test_t))\n",
    "df = []\n",
    "for i in range(len(X_test_t)):\n",
    "    if ((Y_pred[i,1]) < (0.0001)): df.append(0.1)\n",
    "    elif ((Y_pred[i,1]) > (0.99)):df.append(0.99)\n",
    "    else: df.append((Y_pred[i,1]))\n",
    "member_id = test_df['member_id']     \n",
    "dflr = pd.DataFrame( { 'member_id': member_id , 'loan_status': df } )\n",
    "dflr = dfrf[['member_id', 'loan_status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.0604           38.41m\n",
      "         2           1.0366           38.83m\n",
      "         3           1.0233           38.27m\n",
      "         4           1.0127           38.14m\n",
      "         5           1.0037           38.65m\n",
      "         6           0.9900           39.04m\n",
      "         7           0.9827           39.01m\n",
      "         8           0.9765           38.95m\n",
      "         9           0.9712           38.72m\n",
      "        10           0.9616           38.64m\n",
      "        11           0.9568           38.72m\n",
      "        12           0.9485           38.66m\n",
      "        13           0.9446           38.45m\n",
      "        14           0.9408           38.34m\n",
      "        15           0.9376           38.16m\n",
      "        16           0.9347           38.00m\n",
      "        17           0.9318           37.70m\n",
      "        18           0.9254           37.76m\n",
      "        19           0.9229           37.72m\n",
      "        20           0.9206           37.44m\n",
      "        21           0.9186           37.19m\n",
      "        22           0.9144           37.03m\n",
      "        23           0.9125           36.78m\n",
      "        24           0.9097           36.79m\n",
      "        25           0.9080           36.53m\n",
      "        26           0.9046           36.38m\n",
      "        27           0.9031           36.25m\n",
      "        28           0.8994           36.10m\n",
      "        29           0.8978           35.99m\n",
      "        30           0.8955           36.02m\n",
      "        31           0.8942           35.85m\n",
      "        32           0.8929           35.79m\n",
      "        33           0.8903           35.83m\n",
      "        34           0.8892           35.71m\n",
      "        35           0.8868           35.73m\n",
      "        36           0.8857           35.66m\n",
      "        37           0.8830           35.62m\n",
      "        38           0.8817           35.58m\n",
      "        39           0.8804           35.42m\n",
      "        40           0.8784           35.39m\n",
      "        41           0.8773           35.31m\n",
      "        42           0.8761           35.32m\n",
      "        43           0.8741           35.32m\n",
      "        44           0.8695           35.35m\n",
      "        45           0.8685           35.26m\n",
      "        46           0.8665           35.24m\n",
      "        47           0.8640           35.19m\n",
      "        48           0.8622           35.08m\n",
      "        49           0.8614           34.98m\n",
      "        50           0.8571           34.93m\n",
      "        51           0.8563           34.83m\n",
      "        52           0.8523           34.83m\n",
      "        53           0.8515           34.74m\n",
      "        54           0.8504           34.60m\n",
      "        55           0.8499           34.50m\n",
      "        56           0.8484           34.57m\n",
      "        57           0.8475           34.56m\n",
      "        58           0.8460           34.54m\n",
      "        59           0.8453           34.46m\n",
      "        60           0.8447           34.35m\n",
      "        61           0.8408           34.32m\n",
      "        62           0.8397           34.31m\n",
      "        63           0.8392           34.28m\n",
      "        64           0.8387           34.21m\n",
      "        65           0.8381           34.12m\n",
      "        66           0.8376           34.02m\n",
      "        67           0.8370           33.90m\n",
      "        68           0.8366           33.88m\n",
      "        69           0.8355           33.86m\n",
      "        70           0.8343           33.83m\n",
      "        71           0.8329           33.74m\n",
      "        72           0.8303           33.74m\n",
      "        73           0.8286           33.69m\n",
      "        74           0.8281           33.56m\n",
      "        75           0.8257           33.46m\n",
      "        76           0.8245           33.43m\n",
      "        77           0.8235           33.42m\n",
      "        78           0.8225           33.36m\n",
      "        79           0.8213           33.30m\n",
      "        80           0.8186           33.25m\n",
      "        81           0.8177           33.16m\n",
      "        82           0.8167           33.14m\n",
      "        83           0.8155           33.09m\n",
      "        84           0.8129           33.10m\n",
      "        85           0.8118           33.09m\n",
      "        86           0.8106           33.08m\n",
      "        87           0.8093           33.02m\n",
      "        88           0.8069           33.03m\n",
      "        89           0.8062           33.03m\n",
      "        90           0.8042           33.02m\n",
      "        91           0.8031           33.00m\n",
      "        92           0.8023           32.98m\n",
      "        93           0.8019           32.97m\n",
      "        94           0.8011           33.05m\n",
      "        95           0.8008           33.05m\n",
      "        96           0.7997           33.02m\n",
      "        97           0.7991           32.98m\n",
      "        98           0.7977           32.98m\n",
      "        99           0.7969           32.96m\n",
      "       100           0.7955           32.94m\n",
      "       101           0.7945           32.92m\n",
      "       102           0.7938           32.89m\n",
      "       103           0.7923           32.84m\n",
      "       104           0.7909           32.84m\n",
      "       105           0.7902           32.84m\n",
      "       106           0.7894           32.82m\n",
      "       107           0.7886           32.80m\n",
      "       108           0.7880           32.76m\n",
      "       109           0.7870           32.74m\n",
      "       110           0.7866           32.73m\n",
      "       111           0.7864           32.71m\n",
      "       112           0.7854           32.68m\n",
      "       113           0.7847           32.68m\n",
      "       114           0.7842           32.62m\n",
      "       115           0.7831           32.57m\n",
      "       116           0.7824           32.55m\n",
      "       117           0.7820           32.51m\n",
      "       118           0.7811           32.47m\n",
      "       119           0.7800           32.46m\n",
      "       120           0.7792           32.42m\n",
      "       121           0.7789           32.39m\n",
      "       122           0.7779           32.38m\n",
      "       123           0.7774           32.35m\n",
      "       124           0.7768           32.33m\n",
      "       125           0.7761           32.31m\n",
      "       126           0.7753           32.30m\n",
      "       127           0.7743           32.29m\n",
      "       128           0.7738           32.28m\n",
      "       129           0.7732           32.24m\n",
      "       130           0.7722           32.23m\n",
      "       131           0.7718           32.17m\n",
      "       132           0.7715           32.12m\n",
      "       133           0.7711           32.10m\n",
      "       134           0.7708           32.07m\n",
      "       135           0.7704           32.02m\n",
      "       136           0.7701           32.00m\n",
      "       137           0.7696           31.95m\n",
      "       138           0.7690           31.90m\n",
      "       139           0.7686           31.86m\n",
      "       140           0.7680           31.82m\n",
      "       141           0.7675           31.80m\n",
      "       142           0.7672           31.78m\n",
      "       143           0.7669           31.73m\n",
      "       144           0.7666           31.71m\n",
      "       145           0.7661           31.65m\n",
      "       146           0.7656           31.64m\n",
      "       147           0.7654           31.61m\n",
      "       148           0.7648           31.56m\n",
      "       149           0.7639           31.54m\n",
      "       150           0.7631           31.52m\n",
      "       151           0.7625           31.48m\n",
      "       152           0.7617           31.47m\n",
      "       153           0.7612           31.43m\n",
      "       154           0.7601           31.40m\n",
      "       155           0.7597           31.38m\n",
      "       156           0.7590           31.35m\n",
      "       157           0.7586           31.33m\n",
      "       158           0.7580           31.29m\n",
      "       159           0.7576           31.24m\n",
      "       160           0.7570           31.22m\n",
      "       161           0.7562           31.20m\n",
      "       162           0.7560           31.15m\n",
      "       163           0.7554           31.13m\n",
      "       164           0.7548           31.10m\n",
      "       165           0.7546           31.08m\n",
      "       166           0.7541           31.03m\n",
      "       167           0.7538           30.99m\n",
      "       168           0.7536           30.94m\n",
      "       169           0.7532           30.89m\n",
      "       170           0.7528           30.84m\n",
      "       171           0.7521           30.82m\n",
      "       172           0.7514           30.81m\n",
      "       173           0.7509           30.78m\n",
      "       174           0.7506           30.75m\n",
      "       175           0.7504           30.75m\n",
      "       176           0.7502           30.73m\n",
      "       177           0.7496           30.70m\n",
      "       178           0.7488           30.69m\n",
      "       179           0.7486           30.65m\n",
      "       180           0.7484           30.63m\n",
      "       181           0.7482           30.59m\n",
      "       182           0.7476           30.55m\n",
      "       183           0.7470           30.51m\n",
      "       184           0.7466           30.49m\n",
      "       185           0.7459           30.46m\n",
      "       186           0.7450           30.42m\n",
      "       187           0.7445           30.40m\n",
      "       188           0.7440           30.37m\n",
      "       189           0.7434           30.33m\n",
      "       190           0.7428           30.30m\n",
      "       191           0.7425           30.25m\n",
      "       192           0.7423           30.23m\n",
      "       193           0.7415           30.20m\n",
      "       194           0.7410           30.17m\n",
      "       195           0.7405           30.14m\n",
      "       196           0.7403           30.10m\n",
      "       197           0.7399           30.06m\n",
      "       198           0.7391           30.03m\n",
      "       199           0.7386           30.00m\n",
      "       200           0.7384           29.97m\n",
      "       201           0.7381           29.92m\n",
      "       202           0.7378           29.90m\n",
      "       203           0.7375           29.88m\n",
      "       204           0.7373           29.84m\n",
      "       205           0.7371           29.80m\n",
      "       206           0.7369           29.76m\n",
      "       207           0.7368           29.72m\n",
      "       208           0.7367           29.68m\n",
      "       209           0.7365           29.65m\n",
      "       210           0.7361           29.62m\n",
      "       211           0.7353           29.59m\n",
      "       212           0.7349           29.56m\n",
      "       213           0.7345           29.55m\n",
      "       214           0.7342           29.52m\n",
      "       215           0.7337           29.49m\n",
      "       216           0.7335           29.44m\n",
      "       217           0.7333           29.40m\n",
      "       218           0.7330           29.37m\n",
      "       219           0.7327           29.34m\n",
      "       220           0.7325           29.31m\n",
      "       221           0.7323           29.27m\n",
      "       222           0.7321           29.23m\n",
      "       223           0.7318           29.18m\n",
      "       224           0.7316           29.15m\n",
      "       225           0.7315           29.11m\n",
      "       226           0.7310           29.08m\n",
      "       227           0.7307           29.05m\n",
      "       228           0.7305           29.01m\n",
      "       229           0.7303           28.98m\n",
      "       230           0.7302           28.93m\n",
      "       231           0.7300           28.90m\n",
      "       232           0.7298           28.87m\n",
      "       233           0.7291           28.85m\n",
      "       234           0.7288           28.81m\n",
      "       235           0.7287           28.79m\n",
      "       236           0.7286           28.75m\n",
      "       237           0.7284           28.70m\n",
      "       238           0.7282           28.67m\n",
      "       239           0.7279           28.63m\n",
      "       240           0.7276           28.61m\n",
      "       241           0.7272           28.57m\n",
      "       242           0.7269           28.52m\n",
      "       243           0.7266           28.49m\n",
      "       244           0.7264           28.46m\n",
      "       245           0.7260           28.42m\n",
      "       246           0.7254           28.37m\n",
      "       247           0.7250           28.34m\n",
      "       248           0.7248           28.30m\n",
      "       249           0.7246           28.27m\n",
      "       250           0.7241           28.23m\n",
      "       251           0.7235           28.19m\n",
      "       252           0.7231           28.16m\n",
      "       253           0.7229           28.11m\n",
      "       254           0.7226           28.08m\n",
      "       255           0.7224           28.05m\n",
      "       256           0.7218           28.00m\n",
      "       257           0.7213           27.98m\n",
      "       258           0.7208           27.95m\n",
      "       259           0.7206           27.91m\n",
      "       260           0.7202           27.87m\n",
      "       261           0.7197           27.83m\n",
      "       262           0.7195           27.81m\n",
      "       263           0.7192           27.77m\n",
      "       264           0.7189           27.75m\n",
      "       265           0.7185           27.71m\n",
      "       266           0.7183           27.69m\n",
      "       267           0.7180           27.65m\n",
      "       268           0.7174           27.61m\n",
      "       269           0.7172           27.58m\n",
      "       270           0.7170           27.55m\n",
      "       271           0.7167           27.51m\n",
      "       272           0.7164           27.48m\n",
      "       273           0.7158           27.44m\n",
      "       274           0.7155           27.41m\n",
      "       275           0.7154           27.36m\n",
      "       276           0.7152           27.33m\n",
      "       277           0.7151           27.28m\n",
      "       278           0.7147           27.25m\n",
      "       279           0.7146           27.21m\n",
      "       280           0.7141           27.17m\n",
      "       281           0.7139           27.14m\n",
      "       282           0.7137           27.11m\n",
      "       283           0.7134           27.08m\n",
      "       284           0.7133           27.03m\n",
      "       285           0.7126           27.00m\n",
      "       286           0.7124           26.97m\n",
      "       287           0.7122           26.93m\n",
      "       288           0.7120           26.90m\n",
      "       289           0.7119           26.85m\n",
      "       290           0.7115           26.81m\n",
      "       291           0.7112           26.78m\n",
      "       292           0.7110           26.74m\n",
      "       293           0.7108           26.70m\n",
      "       294           0.7105           26.68m\n",
      "       295           0.7102           26.64m\n",
      "       296           0.7096           26.61m\n",
      "       297           0.7094           26.57m\n",
      "       298           0.7092           26.53m\n",
      "       299           0.7089           26.49m\n",
      "       300           0.7087           26.46m\n",
      "       301           0.7084           26.42m\n",
      "       302           0.7082           26.39m\n",
      "       303           0.7079           26.35m\n",
      "       304           0.7076           26.32m\n",
      "       305           0.7074           26.27m\n",
      "       306           0.7068           26.24m\n",
      "       307           0.7066           26.20m\n",
      "       308           0.7062           26.17m\n",
      "       309           0.7060           26.13m\n",
      "       310           0.7058           26.10m\n",
      "       311           0.7055           26.07m\n",
      "       312           0.7052           26.03m\n",
      "       313           0.7050           25.99m\n",
      "       314           0.7045           25.95m\n",
      "       315           0.7043           25.92m\n",
      "       316           0.7039           25.88m\n",
      "       317           0.7036           25.84m\n",
      "       318           0.7034           25.81m\n",
      "       319           0.7032           25.77m\n",
      "       320           0.7028           25.74m\n",
      "       321           0.7024           25.71m\n",
      "       322           0.7020           25.67m\n",
      "       323           0.7018           25.63m\n",
      "       324           0.7017           25.59m\n",
      "       325           0.7016           25.55m\n",
      "       326           0.7015           25.51m\n",
      "       327           0.7011           25.48m\n",
      "       328           0.7008           25.44m\n",
      "       329           0.7006           25.41m\n",
      "       330           0.7004           25.37m\n",
      "       331           0.7001           25.33m\n",
      "       332           0.6997           25.29m\n",
      "       333           0.6994           25.26m\n",
      "       334           0.6992           25.22m\n",
      "       335           0.6991           25.19m\n",
      "       336           0.6987           25.15m\n",
      "       337           0.6984           25.12m\n",
      "       338           0.6982           25.08m\n",
      "       339           0.6979           25.04m\n",
      "       340           0.6976           25.00m\n",
      "       341           0.6974           24.96m\n",
      "       342           0.6972           24.92m\n",
      "       343           0.6971           24.89m\n",
      "       344           0.6968           24.85m\n",
      "       345           0.6963           24.81m\n",
      "       346           0.6962           24.78m\n",
      "       347           0.6960           24.74m\n",
      "       348           0.6959           24.70m\n",
      "       349           0.6957           24.67m\n",
      "       350           0.6956           24.62m\n",
      "       351           0.6955           24.59m\n",
      "       352           0.6954           24.55m\n",
      "       353           0.6951           24.51m\n",
      "       354           0.6947           24.47m\n",
      "       355           0.6945           24.44m\n",
      "       356           0.6944           24.40m\n",
      "       357           0.6943           24.37m\n",
      "       358           0.6941           24.33m\n",
      "       359           0.6939           24.29m\n",
      "       360           0.6936           24.25m\n",
      "       361           0.6934           24.21m\n",
      "       362           0.6934           24.17m\n",
      "       363           0.6932           24.13m\n",
      "       364           0.6930           24.09m\n",
      "       365           0.6927           24.06m\n",
      "       366           0.6923           24.02m\n",
      "       367           0.6921           23.99m\n",
      "       368           0.6917           23.96m\n",
      "       369           0.6915           23.92m\n",
      "       370           0.6911           23.89m\n",
      "       371           0.6908           23.85m\n",
      "       372           0.6906           23.82m\n",
      "       373           0.6903           23.80m\n",
      "       374           0.6901           23.76m\n",
      "       375           0.6899           23.73m\n",
      "       376           0.6896           23.70m\n",
      "       377           0.6895           23.66m\n",
      "       378           0.6892           23.62m\n",
      "       379           0.6891           23.58m\n",
      "       380           0.6889           23.54m\n",
      "       381           0.6888           23.51m\n",
      "       382           0.6886           23.47m\n",
      "       383           0.6885           23.43m\n",
      "       384           0.6882           23.40m\n",
      "       385           0.6880           23.36m\n",
      "       386           0.6878           23.32m\n",
      "       387           0.6876           23.28m\n",
      "       388           0.6875           23.24m\n",
      "       389           0.6873           23.20m\n",
      "       390           0.6872           23.17m\n",
      "       391           0.6868           23.13m\n",
      "       392           0.6867           23.10m\n",
      "       393           0.6864           23.05m\n",
      "       394           0.6862           23.02m\n",
      "       395           0.6861           22.98m\n",
      "       396           0.6860           22.94m\n",
      "       397           0.6858           22.90m\n",
      "       398           0.6856           22.87m\n",
      "       399           0.6853           22.83m\n",
      "       400           0.6851           22.79m\n",
      "       401           0.6850           22.76m\n",
      "       402           0.6849           22.72m\n",
      "       403           0.6847           22.68m\n",
      "       404           0.6845           22.64m\n",
      "       405           0.6841           22.60m\n",
      "       406           0.6840           22.56m\n",
      "       407           0.6836           22.52m\n",
      "       408           0.6835           22.49m\n",
      "       409           0.6832           22.45m\n",
      "       410           0.6831           22.41m\n",
      "       411           0.6827           22.37m\n",
      "       412           0.6826           22.33m\n",
      "       413           0.6825           22.29m\n",
      "       414           0.6823           22.25m\n",
      "       415           0.6820           22.21m\n",
      "       416           0.6819           22.17m\n",
      "       417           0.6817           22.13m\n",
      "       418           0.6816           22.09m\n",
      "       419           0.6814           22.06m\n",
      "       420           0.6813           22.02m\n",
      "       421           0.6810           21.98m\n",
      "       422           0.6808           21.94m\n",
      "       423           0.6807           21.90m\n",
      "       424           0.6805           21.86m\n",
      "       425           0.6801           21.82m\n",
      "       426           0.6799           21.78m\n",
      "       427           0.6798           21.75m\n",
      "       428           0.6796           21.71m\n",
      "       429           0.6796           21.66m\n",
      "       430           0.6795           21.63m\n",
      "       431           0.6794           21.59m\n",
      "       432           0.6793           21.55m\n",
      "       433           0.6792           21.51m\n",
      "       434           0.6792           21.46m\n",
      "       435           0.6790           21.43m\n",
      "       436           0.6788           21.39m\n",
      "       437           0.6786           21.35m\n",
      "       438           0.6785           21.32m\n",
      "       439           0.6783           21.28m\n",
      "       440           0.6780           21.24m\n",
      "       441           0.6779           21.20m\n",
      "       442           0.6778           21.17m\n",
      "       443           0.6776           21.13m\n",
      "       444           0.6773           21.09m\n",
      "       445           0.6772           21.05m\n",
      "       446           0.6770           21.01m\n",
      "       447           0.6768           20.98m\n",
      "       448           0.6766           20.93m\n",
      "       449           0.6765           20.90m\n",
      "       450           0.6763           20.86m\n",
      "       451           0.6761           20.83m\n",
      "       452           0.6759           20.79m\n",
      "       453           0.6757           20.75m\n",
      "       454           0.6755           20.72m\n",
      "       455           0.6754           20.68m\n",
      "       456           0.6752           20.65m\n",
      "       457           0.6751           20.61m\n",
      "       458           0.6751           20.56m\n",
      "       459           0.6750           20.52m\n",
      "       460           0.6748           20.48m\n",
      "       461           0.6746           20.44m\n",
      "       462           0.6745           20.41m\n",
      "       463           0.6743           20.36m\n",
      "       464           0.6741           20.32m\n",
      "       465           0.6740           20.28m\n",
      "       466           0.6740           20.24m\n",
      "       467           0.6739           20.20m\n",
      "       468           0.6736           20.17m\n",
      "       469           0.6736           20.13m\n",
      "       470           0.6734           20.09m\n",
      "       471           0.6733           20.05m\n",
      "       472           0.6730           20.01m\n",
      "       473           0.6729           19.98m\n",
      "       474           0.6728           19.94m\n",
      "       475           0.6726           19.90m\n",
      "       476           0.6725           19.86m\n",
      "       477           0.6724           19.82m\n",
      "       478           0.6722           19.78m\n",
      "       479           0.6719           19.75m\n",
      "       480           0.6718           19.71m\n",
      "       481           0.6716           19.67m\n",
      "       482           0.6715           19.63m\n",
      "       483           0.6713           19.59m\n",
      "       484           0.6712           19.55m\n",
      "       485           0.6711           19.52m\n",
      "       486           0.6709           19.48m\n",
      "       487           0.6708           19.44m\n",
      "       488           0.6707           19.40m\n",
      "       489           0.6706           19.36m\n",
      "       490           0.6704           19.32m\n",
      "       491           0.6701           19.28m\n",
      "       492           0.6700           19.25m\n",
      "       493           0.6698           19.21m\n",
      "       494           0.6697           19.17m\n",
      "       495           0.6696           19.13m\n",
      "       496           0.6693           19.09m\n",
      "       497           0.6692           19.05m\n",
      "       498           0.6690           19.01m\n",
      "       499           0.6690           18.98m\n",
      "       500           0.6688           18.94m\n",
      "       501           0.6687           18.90m\n",
      "       502           0.6685           18.87m\n",
      "       503           0.6684           18.83m\n",
      "       504           0.6683           18.79m\n",
      "       505           0.6681           18.76m\n",
      "       506           0.6679           18.72m\n",
      "       507           0.6677           18.68m\n",
      "       508           0.6676           18.65m\n",
      "       509           0.6675           18.61m\n",
      "       510           0.6674           18.57m\n",
      "       511           0.6670           18.54m\n",
      "       512           0.6669           18.50m\n",
      "       513           0.6666           18.47m\n",
      "       514           0.6666           18.43m\n",
      "       515           0.6665           18.39m\n",
      "       516           0.6663           18.35m\n",
      "       517           0.6662           18.31m\n",
      "       518           0.6661           18.28m\n",
      "       519           0.6660           18.24m\n",
      "       520           0.6659           18.20m\n",
      "       521           0.6658           18.16m\n",
      "       522           0.6657           18.12m\n",
      "       523           0.6656           18.08m\n",
      "       524           0.6656           18.04m\n",
      "       525           0.6653           18.00m\n",
      "       526           0.6652           17.96m\n",
      "       527           0.6651           17.93m\n",
      "       528           0.6649           17.89m\n",
      "       529           0.6647           17.85m\n",
      "       530           0.6646           17.81m\n",
      "       531           0.6644           17.77m\n",
      "       532           0.6643           17.74m\n",
      "       533           0.6642           17.70m\n",
      "       534           0.6640           17.66m\n",
      "       535           0.6639           17.63m\n",
      "       536           0.6638           17.59m\n",
      "       537           0.6636           17.55m\n",
      "       538           0.6635           17.51m\n",
      "       539           0.6632           17.47m\n",
      "       540           0.6631           17.44m\n",
      "       541           0.6629           17.40m\n",
      "       542           0.6628           17.36m\n",
      "       543           0.6626           17.33m\n",
      "       544           0.6625           17.28m\n",
      "       545           0.6624           17.25m\n",
      "       546           0.6623           17.21m\n",
      "       547           0.6622           17.17m\n",
      "       548           0.6620           17.13m\n",
      "       549           0.6619           17.09m\n",
      "       550           0.6618           17.06m\n",
      "       551           0.6617           17.02m\n",
      "       552           0.6615           16.98m\n",
      "       553           0.6614           16.94m\n",
      "       554           0.6614           16.90m\n",
      "       555           0.6612           16.86m\n",
      "       556           0.6611           16.83m\n",
      "       557           0.6611           16.79m\n",
      "       558           0.6610           16.75m\n",
      "       559           0.6608           16.71m\n",
      "       560           0.6606           16.68m\n",
      "       561           0.6604           16.64m\n",
      "       562           0.6603           16.60m\n",
      "       563           0.6602           16.56m\n",
      "       564           0.6601           16.53m\n",
      "       565           0.6600           16.49m\n",
      "       566           0.6599           16.45m\n",
      "       567           0.6598           16.41m\n",
      "       568           0.6597           16.38m\n",
      "       569           0.6596           16.34m\n",
      "       570           0.6594           16.30m\n",
      "       571           0.6593           16.26m\n",
      "       572           0.6591           16.23m\n",
      "       573           0.6591           16.19m\n",
      "       574           0.6588           16.15m\n",
      "       575           0.6587           16.11m\n",
      "       576           0.6586           16.08m\n",
      "       577           0.6585           16.04m\n",
      "       578           0.6584           16.00m\n",
      "       579           0.6581           15.96m\n",
      "       580           0.6580           15.93m\n",
      "       581           0.6579           15.89m\n",
      "       582           0.6578           15.85m\n",
      "       583           0.6577           15.81m\n",
      "       584           0.6575           15.78m\n",
      "       585           0.6574           15.74m\n",
      "       586           0.6572           15.70m\n",
      "       587           0.6571           15.66m\n",
      "       588           0.6570           15.62m\n",
      "       589           0.6569           15.59m\n",
      "       590           0.6568           15.55m\n",
      "       591           0.6567           15.51m\n",
      "       592           0.6565           15.48m\n",
      "       593           0.6563           15.44m\n",
      "       594           0.6562           15.40m\n",
      "       595           0.6562           15.36m\n",
      "       596           0.6560           15.33m\n",
      "       597           0.6560           15.29m\n",
      "       598           0.6559           15.25m\n",
      "       599           0.6559           15.21m\n",
      "       600           0.6557           15.17m\n",
      "       601           0.6556           15.13m\n",
      "       602           0.6555           15.10m\n",
      "       603           0.6554           15.06m\n",
      "       604           0.6553           15.02m\n",
      "       605           0.6552           14.98m\n",
      "       606           0.6552           14.94m\n",
      "       607           0.6551           14.90m\n",
      "       608           0.6550           14.86m\n",
      "       609           0.6549           14.83m\n",
      "       610           0.6548           14.79m\n",
      "       611           0.6547           14.75m\n",
      "       612           0.6546           14.71m\n",
      "       613           0.6545           14.68m\n",
      "       614           0.6544           14.64m\n",
      "       615           0.6544           14.60m\n",
      "       616           0.6543           14.56m\n",
      "       617           0.6542           14.52m\n",
      "       618           0.6541           14.49m\n",
      "       619           0.6540           14.45m\n",
      "       620           0.6539           14.41m\n",
      "       621           0.6538           14.38m\n",
      "       622           0.6537           14.34m\n",
      "       623           0.6536           14.30m\n",
      "       624           0.6534           14.26m\n",
      "       625           0.6534           14.23m\n",
      "       626           0.6533           14.19m\n",
      "       627           0.6531           14.15m\n",
      "       628           0.6530           14.12m\n",
      "       629           0.6529           14.08m\n",
      "       630           0.6527           14.04m\n",
      "       631           0.6527           14.01m\n",
      "       632           0.6526           13.97m\n",
      "       633           0.6526           13.93m\n",
      "       634           0.6525           13.89m\n",
      "       635           0.6525           13.85m\n",
      "       636           0.6524           13.82m\n",
      "       637           0.6523           13.78m\n",
      "       638           0.6522           13.75m\n",
      "       639           0.6520           13.71m\n",
      "       640           0.6519           13.68m\n",
      "       641           0.6518           13.64m\n",
      "       642           0.6517           13.60m\n",
      "       643           0.6515           13.56m\n",
      "       644           0.6514           13.53m\n",
      "       645           0.6513           13.49m\n",
      "       646           0.6511           13.45m\n",
      "       647           0.6510           13.41m\n",
      "       648           0.6509           13.38m\n",
      "       649           0.6508           13.34m\n",
      "       650           0.6506           13.30m\n",
      "       651           0.6505           13.26m\n",
      "       652           0.6504           13.22m\n",
      "       653           0.6504           13.18m\n",
      "       654           0.6502           13.14m\n",
      "       655           0.6502           13.11m\n",
      "       656           0.6500           13.07m\n",
      "       657           0.6500           13.03m\n",
      "       658           0.6499           12.99m\n",
      "       659           0.6497           12.95m\n",
      "       660           0.6496           12.91m\n",
      "       661           0.6496           12.87m\n",
      "       662           0.6494           12.83m\n",
      "       663           0.6493           12.80m\n",
      "       664           0.6492           12.76m\n",
      "       665           0.6490           12.72m\n",
      "       666           0.6489           12.69m\n",
      "       667           0.6489           12.65m\n",
      "       668           0.6487           12.61m\n",
      "       669           0.6487           12.57m\n",
      "       670           0.6486           12.54m\n",
      "       671           0.6485           12.50m\n",
      "       672           0.6482           12.46m\n",
      "       673           0.6481           12.42m\n",
      "       674           0.6480           12.39m\n",
      "       675           0.6478           12.35m\n",
      "       676           0.6476           12.31m\n",
      "       677           0.6476           12.27m\n",
      "       678           0.6475           12.23m\n",
      "       679           0.6474           12.19m\n",
      "       680           0.6473           12.15m\n",
      "       681           0.6473           12.11m\n",
      "       682           0.6471           12.08m\n",
      "       683           0.6470           12.04m\n",
      "       684           0.6469           11.99m\n",
      "       685           0.6469           11.95m\n",
      "       686           0.6468           11.91m\n",
      "       687           0.6467           11.87m\n",
      "       688           0.6466           11.83m\n",
      "       689           0.6465           11.79m\n",
      "       690           0.6464           11.75m\n",
      "       691           0.6463           11.71m\n",
      "       692           0.6462           11.67m\n",
      "       693           0.6462           11.63m\n",
      "       694           0.6461           11.59m\n",
      "       695           0.6459           11.55m\n",
      "       696           0.6458           11.51m\n",
      "       697           0.6456           11.47m\n",
      "       698           0.6455           11.43m\n",
      "       699           0.6454           11.39m\n",
      "       700           0.6452           11.35m\n",
      "       701           0.6451           11.32m\n",
      "       702           0.6450           11.28m\n",
      "       703           0.6449           11.24m\n",
      "       704           0.6448           11.20m\n",
      "       705           0.6447           11.16m\n",
      "       706           0.6446           11.12m\n",
      "       707           0.6445           11.08m\n",
      "       708           0.6444           11.04m\n",
      "       709           0.6443           11.00m\n",
      "       710           0.6442           10.96m\n",
      "       711           0.6441           10.92m\n",
      "       712           0.6441           10.88m\n",
      "       713           0.6439           10.84m\n",
      "       714           0.6439           10.80m\n",
      "       715           0.6436           10.76m\n",
      "       716           0.6436           10.72m\n",
      "       717           0.6435           10.68m\n",
      "       718           0.6434           10.64m\n",
      "       719           0.6432           10.60m\n",
      "       720           0.6431           10.56m\n",
      "       721           0.6430           10.52m\n",
      "       722           0.6428           10.48m\n",
      "       723           0.6427           10.44m\n",
      "       724           0.6427           10.40m\n",
      "       725           0.6426           10.36m\n",
      "       726           0.6425           10.32m\n",
      "       727           0.6424           10.28m\n",
      "       728           0.6423           10.24m\n",
      "       729           0.6423           10.21m\n",
      "       730           0.6421           10.17m\n",
      "       731           0.6421           10.13m\n",
      "       732           0.6420           10.09m\n",
      "       733           0.6420           10.05m\n",
      "       734           0.6419           10.02m\n",
      "       735           0.6418            9.98m\n",
      "       736           0.6418            9.94m\n",
      "       737           0.6417            9.90m\n",
      "       738           0.6417            9.87m\n",
      "       739           0.6416            9.83m\n",
      "       740           0.6416            9.79m\n",
      "       741           0.6415            9.75m\n",
      "       742           0.6415            9.71m\n",
      "       743           0.6414            9.68m\n",
      "       744           0.6412            9.64m\n",
      "       745           0.6412            9.60m\n",
      "       746           0.6411            9.57m\n",
      "       747           0.6410            9.53m\n",
      "       748           0.6409            9.49m\n",
      "       749           0.6408            9.46m\n",
      "       750           0.6407            9.42m\n",
      "       751           0.6407            9.38m\n",
      "       752           0.6406            9.34m\n",
      "       753           0.6405            9.30m\n",
      "       754           0.6404            9.27m\n",
      "       755           0.6403            9.23m\n",
      "       756           0.6402            9.20m\n",
      "       757           0.6401            9.16m\n",
      "       758           0.6400            9.12m\n",
      "       759           0.6399            9.09m\n",
      "       760           0.6398            9.05m\n",
      "       761           0.6397            9.01m\n",
      "       762           0.6397            8.97m\n",
      "       763           0.6396            8.94m\n",
      "       764           0.6395            8.90m\n",
      "       765           0.6394            8.86m\n",
      "       766           0.6392            8.82m\n",
      "       767           0.6391            8.79m\n",
      "       768           0.6391            8.75m\n",
      "       769           0.6390            8.71m\n",
      "       770           0.6390            8.67m\n",
      "       771           0.6388            8.64m\n",
      "       772           0.6387            8.60m\n",
      "       773           0.6387            8.56m\n",
      "       774           0.6386            8.52m\n",
      "       775           0.6385            8.48m\n",
      "       776           0.6384            8.45m\n",
      "       777           0.6383            8.41m\n",
      "       778           0.6382            8.37m\n",
      "       779           0.6381            8.33m\n",
      "       780           0.6380            8.29m\n",
      "       781           0.6379            8.26m\n",
      "       782           0.6379            8.22m\n",
      "       783           0.6378            8.18m\n",
      "       784           0.6378            8.14m\n",
      "       785           0.6377            8.10m\n",
      "       786           0.6377            8.07m\n",
      "       787           0.6376            8.03m\n",
      "       788           0.6376            7.99m\n",
      "       789           0.6375            7.95m\n",
      "       790           0.6374            7.91m\n",
      "       791           0.6373            7.88m\n",
      "       792           0.6372            7.84m\n",
      "       793           0.6371            7.80m\n",
      "       794           0.6370            7.77m\n",
      "       795           0.6369            7.73m\n",
      "       796           0.6369            7.69m\n",
      "       797           0.6368            7.65m\n",
      "       798           0.6367            7.62m\n",
      "       799           0.6366            7.58m\n",
      "       800           0.6364            7.54m\n",
      "       801           0.6364            7.50m\n",
      "       802           0.6363            7.47m\n",
      "       803           0.6362            7.43m\n",
      "       804           0.6361            7.39m\n",
      "       805           0.6361            7.35m\n",
      "       806           0.6360            7.32m\n",
      "       807           0.6359            7.28m\n",
      "       808           0.6359            7.24m\n",
      "       809           0.6358            7.20m\n",
      "       810           0.6357            7.17m\n",
      "       811           0.6356            7.13m\n",
      "       812           0.6355            7.09m\n",
      "       813           0.6355            7.05m\n",
      "       814           0.6354            7.01m\n",
      "       815           0.6354            6.98m\n",
      "       816           0.6354            6.94m\n",
      "       817           0.6353            6.90m\n",
      "       818           0.6352            6.86m\n",
      "       819           0.6350            6.82m\n",
      "       820           0.6350            6.79m\n",
      "       821           0.6350            6.75m\n",
      "       822           0.6349            6.71m\n",
      "       823           0.6349            6.67m\n",
      "       824           0.6348            6.63m\n",
      "       825           0.6347            6.60m\n",
      "       826           0.6346            6.56m\n",
      "       827           0.6346            6.52m\n",
      "       828           0.6345            6.48m\n",
      "       829           0.6344            6.45m\n",
      "       830           0.6344            6.41m\n",
      "       831           0.6343            6.37m\n",
      "       832           0.6342            6.33m\n",
      "       833           0.6340            6.30m\n",
      "       834           0.6340            6.26m\n",
      "       835           0.6339            6.22m\n",
      "       836           0.6338            6.18m\n",
      "       837           0.6337            6.14m\n",
      "       838           0.6337            6.11m\n",
      "       839           0.6336            6.07m\n",
      "       840           0.6336            6.03m\n",
      "       841           0.6335            5.99m\n",
      "       842           0.6335            5.96m\n",
      "       843           0.6334            5.92m\n",
      "       844           0.6334            5.88m\n",
      "       845           0.6334            5.84m\n",
      "       846           0.6332            5.80m\n",
      "       847           0.6332            5.77m\n",
      "       848           0.6332            5.73m\n",
      "       849           0.6331            5.69m\n",
      "       850           0.6330            5.65m\n",
      "       851           0.6330            5.61m\n",
      "       852           0.6329            5.58m\n",
      "       853           0.6329            5.54m\n",
      "       854           0.6328            5.50m\n",
      "       855           0.6327            5.46m\n",
      "       856           0.6326            5.43m\n",
      "       857           0.6325            5.39m\n",
      "       858           0.6324            5.35m\n",
      "       859           0.6323            5.31m\n",
      "       860           0.6322            5.28m\n",
      "       861           0.6320            5.24m\n",
      "       862           0.6320            5.20m\n",
      "       863           0.6319            5.16m\n",
      "       864           0.6318            5.13m\n",
      "       865           0.6317            5.09m\n",
      "       866           0.6316            5.05m\n",
      "       867           0.6315            5.01m\n",
      "       868           0.6314            4.98m\n",
      "       869           0.6313            4.94m\n",
      "       870           0.6313            4.90m\n",
      "       871           0.6312            4.86m\n",
      "       872           0.6312            4.82m\n",
      "       873           0.6312            4.79m\n",
      "       874           0.6311            4.75m\n",
      "       875           0.6311            4.71m\n",
      "       876           0.6310            4.67m\n",
      "       877           0.6310            4.64m\n",
      "       878           0.6309            4.60m\n",
      "       879           0.6308            4.56m\n",
      "       880           0.6308            4.52m\n",
      "       881           0.6307            4.48m\n",
      "       882           0.6306            4.45m\n",
      "       883           0.6306            4.41m\n",
      "       884           0.6305            4.37m\n",
      "       885           0.6304            4.33m\n",
      "       886           0.6303            4.30m\n",
      "       887           0.6303            4.26m\n",
      "       888           0.6301            4.22m\n",
      "       889           0.6301            4.18m\n",
      "       890           0.6300            4.15m\n",
      "       891           0.6299            4.11m\n",
      "       892           0.6299            4.07m\n",
      "       893           0.6298            4.03m\n",
      "       894           0.6298            4.00m\n",
      "       895           0.6297            3.96m\n",
      "       896           0.6297            3.92m\n",
      "       897           0.6295            3.88m\n",
      "       898           0.6294            3.84m\n",
      "       899           0.6294            3.81m\n",
      "       900           0.6294            3.77m\n",
      "       901           0.6293            3.73m\n",
      "       902           0.6293            3.69m\n",
      "       903           0.6292            3.66m\n",
      "       904           0.6292            3.62m\n",
      "       905           0.6291            3.58m\n",
      "       906           0.6290            3.54m\n",
      "       907           0.6289            3.51m\n",
      "       908           0.6288            3.47m\n",
      "       909           0.6287            3.43m\n",
      "       910           0.6286            3.39m\n",
      "       911           0.6285            3.36m\n",
      "       912           0.6285            3.32m\n",
      "       913           0.6284            3.28m\n",
      "       914           0.6283            3.24m\n",
      "       915           0.6283            3.20m\n",
      "       916           0.6282            3.17m\n",
      "       917           0.6281            3.13m\n",
      "       918           0.6281            3.09m\n",
      "       919           0.6280            3.05m\n",
      "       920           0.6279            3.02m\n",
      "       921           0.6279            2.98m\n",
      "       922           0.6279            2.94m\n",
      "       923           0.6278            2.90m\n",
      "       924           0.6277            2.87m\n",
      "       925           0.6276            2.83m\n",
      "       926           0.6275            2.79m\n",
      "       927           0.6275            2.75m\n",
      "       928           0.6275            2.71m\n",
      "       929           0.6274            2.68m\n",
      "       930           0.6274            2.64m\n",
      "       931           0.6274            2.60m\n",
      "       932           0.6273            2.56m\n",
      "       933           0.6272            2.53m\n",
      "       934           0.6271            2.49m\n",
      "       935           0.6270            2.45m\n",
      "       936           0.6270            2.41m\n",
      "       937           0.6269            2.37m\n",
      "       938           0.6269            2.34m\n",
      "       939           0.6268            2.30m\n",
      "       940           0.6267            2.26m\n",
      "       941           0.6267            2.22m\n",
      "       942           0.6266            2.19m\n",
      "       943           0.6265            2.15m\n",
      "       944           0.6265            2.11m\n",
      "       945           0.6264            2.07m\n",
      "       946           0.6263            2.04m\n",
      "       947           0.6263            2.00m\n",
      "       948           0.6262            1.96m\n",
      "       949           0.6262            1.92m\n",
      "       950           0.6260            1.89m\n",
      "       951           0.6259            1.85m\n",
      "       952           0.6259            1.81m\n",
      "       953           0.6258            1.77m\n",
      "       954           0.6257            1.74m\n",
      "       955           0.6257            1.70m\n",
      "       956           0.6256            1.66m\n",
      "       957           0.6256            1.62m\n",
      "       958           0.6255            1.58m\n",
      "       959           0.6255            1.55m\n",
      "       960           0.6254            1.51m\n",
      "       961           0.6253            1.47m\n",
      "       962           0.6253            1.43m\n",
      "       963           0.6251            1.40m\n",
      "       964           0.6251            1.36m\n",
      "       965           0.6251            1.32m\n",
      "       966           0.6251            1.28m\n",
      "       967           0.6250            1.24m\n",
      "       968           0.6250            1.21m\n",
      "       969           0.6250            1.17m\n",
      "       970           0.6248            1.13m\n",
      "       971           0.6248            1.09m\n",
      "       972           0.6248            1.06m\n",
      "       973           0.6248            1.02m\n",
      "       974           0.6247           58.83s\n",
      "       975           0.6246           56.56s\n",
      "       976           0.6246           54.31s\n",
      "       977           0.6245           52.05s\n",
      "       978           0.6245           49.78s\n",
      "       979           0.6244           47.52s\n",
      "       980           0.6244           45.26s\n",
      "       981           0.6243           43.00s\n",
      "       982           0.6242           40.73s\n",
      "       983           0.6241           38.47s\n",
      "       984           0.6240           36.21s\n",
      "       985           0.6240           33.95s\n",
      "       986           0.6240           31.69s\n",
      "       987           0.6239           29.43s\n",
      "       988           0.6239           27.16s\n",
      "       989           0.6238           24.90s\n",
      "       990           0.6237           22.63s\n",
      "       991           0.6236           20.37s\n",
      "       992           0.6235           18.11s\n",
      "       993           0.6235           15.84s\n",
      "       994           0.6234           13.58s\n",
      "       995           0.6233           11.32s\n",
      "       996           0.6233            9.06s\n",
      "       997           0.6232            6.79s\n",
      "       998           0.6231            4.53s\n",
      "       999           0.6231            2.26s\n",
      "      1000           0.6230            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87.254"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfgb = GradientBoostingClassifier(n_estimators=1000, max_depth=3, verbose=2)\n",
    "clfgb.fit(X_train, Y_train)\n",
    "acc_log = round(clfgb.score(X_test,Y_test) * 100,3)\n",
    "acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test_t=test_df.drop([\"member_id\"], axis=1).copy()\n",
    "Y_pred = (clfgb.predict_proba(X_test_t))\n",
    "df = []\n",
    "for i in range(len(X_test_t)):\n",
    "    if ((Y_pred[i,1]) < (0.0001)): df.append(0.1)\n",
    "    elif ((Y_pred[i,1]) > (0.99)):df.append(0.99)\n",
    "    else: df.append((Y_pred[i,1]))\n",
    "member_id = test_df['member_id']     \n",
    "dfgb = pd.DataFrame( { 'member_id': member_id , 'loan_status': df } )\n",
    "dfgb = dfrf[['member_id', 'loan_status']]\n",
    "dfgb.to_csv( 'loansub.csv', index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final2 = pd.DataFrame()\n",
    "member_id2= test_df['member_id']\n",
    "\n",
    "df_final2['loan_status'] = pd.DataFrame((dfETC['loan_status'])*2+(dfmlp['loan_status'])*1 +(dfrf['loan_status'])*4 + (dfxbg['loan_status'])*4)/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_final2=pd.DataFrame({ 'member_id': member_id , 'loan_status': df_final2['loan_status']  } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_final = df_final2 [['member_id', 'loan_status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.549510e+05</td>\n",
       "      <td>354951.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.499635e+07</td>\n",
       "      <td>0.206285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.410120e+07</td>\n",
       "      <td>0.164157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.062600e+04</td>\n",
       "      <td>0.010498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.088941e+07</td>\n",
       "      <td>0.088519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.708650e+07</td>\n",
       "      <td>0.149144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.844892e+07</td>\n",
       "      <td>0.271878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.354483e+07</td>\n",
       "      <td>0.911158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          member_id    loan_status\n",
       "count  3.549510e+05  354951.000000\n",
       "mean   3.499635e+07       0.206285\n",
       "std    2.410120e+07       0.164157\n",
       "min    7.062600e+04       0.010498\n",
       "25%    1.088941e+07       0.088519\n",
       "50%    3.708650e+07       0.149144\n",
       "75%    5.844892e+07       0.271878\n",
       "max    7.354483e+07       0.911158"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_final.to_csv( 'loan_avangers.csv', index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
